<a name="9EXq5"></a>
## 理论知识
<a name="ycIq0"></a>
#### 应用

- 早期应用：垃圾邮件分类
- 图像识别：两只动物，如何识别狗、猫
- 人脸识别：AI换脸，智能马赛克
- 数字识别：快递码识别



<a name="NWOod"></a>
#### 人类如何学习

<br />![](https://cdn.nlark.com/yuque/0/2021/png/95500/1623855023510-6fdfa8e7-8abe-4720-af48-d9906a4fcd38.png#align=left&display=inline&height=189&margin=%5Bobject%20Object%5D&originHeight=758&originWidth=2164&status=done&style=none&width=540)<br />

<a name="nbxy1"></a>
#### 机器如何学习

<br />![](https://cdn.nlark.com/yuque/0/2021/png/95500/1623855061254-60e169ba-9375-4cc2-8878-a8c7b5cb7e53.png#align=left&display=inline&height=189&margin=%5Bobject%20Object%5D&originHeight=758&originWidth=2168&status=done&style=none&width=540)<br />**
<a name="V9C0Z"></a>
#### 机器学习分类：
监督学习
> 给机器的训练数据拥有“标记”或者“答案”（分类问题和回归问题，例如垃圾邮件的判断）

非监督学习
> 给机器的训练数据没有任何“标记”或者“答案”，学习模型是为了推断出数据的一些内在结（对数据进行分类-聚类分析）

半监督学习
> 一部分数据有“标记”或者“答案”，另一部分数据没有，可以用来进行预测（分类和回归）

增强(强化)学习
> 根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式，输入数据直接反馈到模型，模型必须对此立刻作出调整（动态系统以及机器人控制）

<a name="98Zw8"></a>
#### 常见的算法

- KNN
- SVM
- 线性回归
- 决策树
- 多项线性回归
- 随机森林
- 逻辑回归
<a name="9faTg"></a>
#### 技术栈

- 语言：Python3
- 框架：Scikit-learn
- 其他：numpy，matplotlib， panda……
- IDE：Jupyter Notebook
<a name="a5XD9"></a>
## 基本数据处理
<a name="FFD3S"></a>
### Numpy
[https://github.com/xiaoxie110/python_learn/blob/main/numpy.ipynb](https://github.com/xiaoxie110/python_learn/blob/main/numpy.ipynb)
<a name="4UxPW"></a>
### Matplotlib
[https://github.com/xiaoxie110/python_learn/blob/main/matplotlib.ipynb](https://github.com/xiaoxie110/python_learn/blob/main/matplotlib.ipynb)
<a name="puJeE"></a>
## KNN算法-最基础的分类算法
<a name="fSlm6"></a>
### 算法介绍
KNN的全称是K Nearest Neighbors，K个最近邻居，毫无疑问，K的取值肯定是至关重要的。那么最近的邻居又是怎么回事呢？**其实啊，KNN的原理就是当预测一个新的值x的时候，根据它距离最近的K个点是什么类别来判断x属于哪个类别**。<br />![](https://cdn.nlark.com/yuque/0/2021/png/1785332/1628094858667-951792b8-53cf-4c1b-b5da-62d94674395e.png#align=left&display=inline&height=378&margin=%5Bobject%20Object%5D&originHeight=378&originWidth=844&size=0&status=done&style=none&width=844)<br />图中绿色的点就是我们要预测的那个点，假设K=3。那么KNN算法就会找到与它距离最近的三个点（这里用圆圈把它圈起来了），看看哪种类别多一些，比如这个例子中是蓝色三角形多一些，新来的绿色点就归类到蓝三角了。<br />![](https://cdn.nlark.com/yuque/0/2021/png/1785332/1628094858634-0d1e6621-f679-4313-b686-af473a50963d.png#align=left&display=inline&height=380&margin=%5Bobject%20Object%5D&originHeight=380&originWidth=919&size=0&status=done&style=none&width=919)<br />但是，**当K=5的时候，判定就变成不一样了**。这次变成红圆多一些，所以新来的绿点被归类成红圆。从这个例子中，我们就能看得出K的取值是很重要的。<br />明白了大概原理后，我们来说一下具体实现，**KNN算法实现有四个步骤：特征工程、样本标注、相似度计算（距离计算）、K值的选取**<br />明白了大概原理后，我们就来说一说细节的东西吧，主要有两个，**K值的选取**和**点距离的计算**。
<a name="ka2q4"></a>
### 算法原理
由于sklearn已经封装好了KNN算法，所以用起来很简单。但它的具体的实现细节又如何呢？<br />为了实现一个KNN算法，我们需要具备四个方面的信息。
<a name="y89Or"></a>
#### 特征工程
第一、特征工程，把一个物体表示成向量、矩阵、张量等数量化的信息。任何的算法的输入一定是数量化的信息，我们把它叫做特征，需要把现实生活中的物体通过数字化的特征来进行描述。
> 


![](https://cdn.nlark.com/yuque/0/2021/jpeg/1785332/1628390920751-585eafe7-f978-4e0a-b5da-3a61704dd95f.jpeg#align=left&display=inline&height=419&margin=%5Bobject%20Object%5D&originHeight=927&originWidth=1593&size=0&status=done&style=none&width=720)
<a name="tdKoW"></a>
#### 样本标注
第二、由于KNN是监督学习算法，所以需要提前标注好的样本。<br />![](https://cdn.nlark.com/yuque/0/2021/jpeg/1785332/1628390920717-b42752c9-af6d-4ca1-9a03-3b414b59a4a5.jpeg#align=left&display=inline&height=424&margin=%5Bobject%20Object%5D&originHeight=424&originWidth=720&size=0&status=done&style=none&width=720)
<a name="ECsZ7"></a>
#### 相似度计算
第三、我们需要想办法来计算两个样本之间的距离或者相似度，之后才能选出最相近的样本。比如常见的曼哈顿距离计算，欧式距离计算等等。不过通常KNN算法中使用的是欧式距离，这里只是简单说一下，拿二维平面为例，，二维空间两个点的欧式距离计算公式如下：<br />![](https://cdn.nlark.com/yuque/0/2021/jpeg/1785332/1628094858681-97f9b2ce-423f-47e7-a46b-f1b1b001493a.jpeg#align=left&display=inline&height=32&margin=%5Bobject%20Object%5D&originHeight=32&originWidth=191&size=0&status=done&style=none&width=191)<br />这个高中应该就有接触到的了，其实就是计算（x1,y1）和（x2,y2）的距离。拓展到多维空间，则公式变成这样：<br />![](https://cdn.nlark.com/yuque/0/2021/jpeg/1785332/1628094858660-1dce04c3-9ab3-488c-9ea0-bf2ca68bf338.jpeg#align=left&display=inline&height=56&margin=%5Bobject%20Object%5D&originHeight=56&originWidth=469&size=0&status=done&style=none&width=469)<br />![](https://cdn.nlark.com/yuque/0/2021/jpeg/1785332/1628390920802-46bfe5d1-4ebd-4996-961c-df861620aadd.jpeg#align=left&display=inline&height=413&margin=%5Bobject%20Object%5D&originHeight=413&originWidth=720&size=0&status=done&style=none&width=720)
> 距离的计算很容易受到某个值的有影响，一般会采用特征缩放解决KNN算法的潜在隐患，分别是线性归一化和标准差归一化。其中，线性归一化指的是把特征值的范围映射到[0,1]区间，标准差标准化的方法使得把特征值映射到均值为0，标准差为1的正态分布。

<a name="py8mO"></a>
#### 选择最合适的K值
通过上面那张图我们知道K的取值比较重要，那么该如何确定K取多少值好呢？答案是通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值。<br />通过交叉验证计算方差后你大致会得到下面这样的图：<br />![](https://cdn.nlark.com/yuque/0/2021/png/1785332/1628094858791-4bc2ad8f-c591-435d-913d-12a62061f14b.png#align=left&display=inline&height=335&margin=%5Bobject%20Object%5D&originHeight=374&originWidth=805&size=0&status=done&style=none&width=720)<br />这个图其实很好理解，当你增大k的时候，一般错误率会先降低，因为有周围更多的样本可以借鉴了，分类效果会变好。但注意，和K-means不一样，当K值更大的时候，错误率会更高。这也很好理解，比如说你一共就35个样本，当你K增大到30的时候，KNN基本上就没意义了。<br />所以选择K点的时候可以选择一个较大的临界K点，当它继续增大或减小的时候，错误率都会上升，比如图中的K=10。具体如何得出K最佳值的代码，大家有兴趣的话可以自行去研究。
<a name="Ks3YM"></a>
### 算法实现

<br />![](https://cdn.nlark.com/yuque/0/2021/png/1785332/1628325115360-e15a0db0-72f1-45a7-a1a9-64c93659f317.png#align=left&display=inline&height=270&margin=%5Bobject%20Object%5D&originHeight=658&originWidth=1757&size=0&status=done&style=none&width=720)<br />上面数据集中序号1-10为已知的电影分类，分为喜剧片、动作片、爱情片三个种类，使用的特征值分别为搞笑镜头、打斗镜头、拥抱镜头的数量。那么来了一部新电影《唐人街探案》，它属于上述3个电影分类中的哪个类型？用KNN是怎么做的呢？<br />[https://github.com/xiaoxie110/python_learn/blob/main/knn.ipynb](https://github.com/xiaoxie110/python_learn/blob/main/knn.ipynb)
<a name="o1NzF"></a>
### KNN算法总结
KNN 是一个简单的**无显示学习过程，非泛化学习的监督学习**模型。在分类和回归中均有应用。 简单来说： 通过距离度量来计算查询点（query point）与每个训练数据点的距离，然后选出与查询点（query point）相近的K个最邻点（K nearest neighbors），使用分类决策来选出对应的标签来作为该查询点的标签
<a name="j05Wp"></a>
#### KNN算法优点

1. 简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。
1. 模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。
1. 预测效果好。
1. 对异常值不敏感
<a name="vzDqv"></a>
#### KNN算法缺点

1. 对内存要求较高，因为该算法存储了所有训练数据
1. 预测阶段可能很慢
1. 对不相关的功能和数据规模敏感

机器学习的两个关键有因素：数据的重要性：**数据即算法-数据驱动；**算法的选择：**算法选择-简单就是好的，特定问题特定算法**
